{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Callable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary of sessions 1 and 2\n",
    "\n",
    "In the first session we came up with a simple problem that nevertheless captures many of ideas involved in\n",
    "modern machine learning: finding the optimal coefficients of a linear model for predicting the math grade\n",
    "of a student from the time they spent studying in the lyzeum2."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We saw that generally for such problems, we will need to find the minima (or maxima) of rather\n",
    "complicated functions. This brought us to the topic of _optimization_. Sometimes, for simple problems,\n",
    "optimization can be performed analytically - this means finding an exact solution by solving a set of equations.\n",
    "However, for the majority of problems the optimality equations are way too complicated to be addressed analytically,\n",
    "and we need to resort to _numerical optimization_. This means finding the optimal values through a program - a\n",
    "sequence of calculations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We sketched our first program for solving a very simplified optimization - finding the minimum of a one dimensional\n",
    "function. The sketch evolved around the following idea:\n",
    "\n",
    "Given a function of a single real variable $f: \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
    "\n",
    "1. Select some initial value $x=x_0$ (either randomly or fixed)\n",
    "2. Select two values of $x$, one to the left, one to the right with a fixed step-size $\\Delta$, i.e.\n",
    "   $x-\\Delta$, $x+\\Delta$. Compute $f(x), \\ f(x-\\Delta), \\ f(x+\\Delta)$.\n",
    "3. If $f(x)$ is the lowest among them, terminate the algorithm and report $x, \\ f(x)$ as min and argmin.\n",
    "   Otherwise, go either to $x-\\Delta$ or $x+\\Delta$, depending on where $f$ was smaller.\n",
    "4. Continue with step $2$ either until the algorithm has stopped (we call that _convergence_) or the maximal amount\n",
    "   of steps is reached.\n",
    "\n",
    "Here is what an implementation of the algorithm looks like in python"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def greedy_1dim_search(\n",
    "        f,\n",
    "        num_steps=50,\n",
    "        step_size=0.1,\n",
    "        initial_x=0\n",
    "):\n",
    "    current_pos = initial_x\n",
    "    current_value = f(initial_x)\n",
    "    for n in range(num_steps):\n",
    "        step_left, step_right = current_pos - step_size, current_pos + step_size\n",
    "        value_left, value_right = f(step_left), f(step_right)\n",
    "        if min(value_left, value_right) >= current_value:\n",
    "            print(f\"Converged after {n} steps\")\n",
    "            return current_pos, current_value\n",
    "\n",
    "        if value_left < value_right:\n",
    "            current_pos = step_left\n",
    "            current_value = value_left\n",
    "        else:\n",
    "            current_pos = step_right\n",
    "            current_value = value_right\n",
    "    print(f\"Failed to converge after {num_steps} steps, returning current candidates for argmin and min\")\n",
    "    return current_pos, current_value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us try it out on a bunch of functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return (x-2)**2 + 10\n",
    "\n",
    "def f2(x):\n",
    "    return (x - 1.05)**4 - x ** 3\n",
    "\n",
    "def f3(x):\n",
    "    return (x-10) ** 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for function f1\n",
      "Converged after 20 steps\n",
      "Min found at 2.0000000000000004 with value 10.0\n",
      "\n",
      "Result for function f2\n",
      "Converged after 29 steps\n",
      "Min found at 2.9000000000000012 with value -12.675493750000001\n",
      "\n",
      "Result for function f3\n",
      "Failed to converge after 50 steps, returning current candidates for argmin and min\n",
      "Min found at 4.999999999999998 with value 25.000000000000018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for f in [f1, f2, f3]:\n",
    "    print(f\"Result for function {f.__name__}\")\n",
    "    print(\"Min found at {} with value {}\".format(*greedy_1dim_search(f)))\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Note:\n",
    "\n",
    "There might have been some syntax that you haven't seen before in the above cell. We looped over\n",
    "a list of functions, used python's inbuilt field `__name__` to extract their names, used _old-style string formatting_\n",
    "(the new style is using f\"...\" but it would have been more cumbersome here) and used variable unpacking with `*`.\n",
    "This is not super important for you right now, but you still should read up on these topics and the syntax to get\n",
    "more acquainted with python. Just googling and trying things out on your own in a notebook is a good way to go here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A note on software engineering\n",
    "\n",
    "Our implementation of `greedy_1dim_search` works but it is not very well written. We discussed the following questions:\n",
    "what are the parameters that `greedy_1dim_search` can accept? What does it return?\n",
    "\n",
    "Python is a _weakly typed_ language, one can entirely omit type information. But it makes the program difficult to read\n",
    "and also difficult to use correctly if such information is missing. How do we know that `f` should be a function?\n",
    "What kind of function can it be? How do we know that `greedy_1dim_search` returns a tuple of two floats? This kind of\n",
    "information is called _signature_ of a function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In strongly typed languages like Java, C/C++ and others such type information has to be added. It is then checked\n",
    "when the program is compiled (which happens before it actually runs). In python, one should use the `typing` module\n",
    "instead to leave hints about the input and return types. These hints are mostly ignored at runtime and thus ado not add\n",
    "any functionality (there are some exceptions). However, a good programming environment, like PyCharm or VSCode,\n",
    "will use a type checker to warn you if you try to pass incompatible types. This will make your life much easier and\n",
    "type hints should therefore always be used! Here is what our function would look like with type hints"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def greedy_1dim_search(f: Callable[[float], float],\n",
    "                       num_steps=50,\n",
    "                       step_size=0.1,\n",
    "                       initial_x: float = 0) -> Tuple[float, float]:\n",
    "    current_pos = initial_x\n",
    "    current_value = f(initial_x)\n",
    "    for n in range(num_steps):\n",
    "        step_left, step_right = current_pos - step_size, current_pos + step_size\n",
    "        value_left, value_right = f(step_left), f(step_right)\n",
    "        if min(value_left, value_right) >= current_value:\n",
    "            print(f\"Converged after {n} steps\")\n",
    "            return current_pos, current_value\n",
    "\n",
    "        if value_left < value_right:\n",
    "            current_pos = step_left\n",
    "            current_value = value_left\n",
    "        else:\n",
    "            current_pos = step_right\n",
    "            current_value = value_right\n",
    "    print(f\"Failed to converge after {num_steps} steps, returning current candidates for argmin and min\")\n",
    "    return current_pos, current_value\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you see, nothing has changed in the implementation, only the declaration of the function looks different. Make sure\n",
    "you google a bit and understand the `Callable` type and all the other ones as well."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Drawbacks of the greedy search\n",
    "\n",
    "We discussed problems with the above algorithm and also how to extend it to higher dimensions. The main problems we\n",
    "identified were:\n",
    "\n",
    "1. We compute `f` multiple times on the same value. This can easily be fixed in 1 dim. but is harder to fix in more.\n",
    "2. Because of the fixed step size, we will not find the actual minimum if it does not lie an integer number of step\n",
    "   sizes away from our initial $x$. This happened for `f2` above.\n",
    "3. When we are far from the optimum, we need more steps. If we increase the step size, we will have fewer steps but\n",
    "also lose precision (see point 2 above). This happened for `f3` above.\n",
    "\n",
    "### Homework 1:\n",
    "For those missing last time can you see more problems with it? Try to adjust the algorithm such that it works in more\n",
    "dimensions, i.e. when f is of the type `f: Callable[[float, ...], float]` - a function with an unknown number of\n",
    "arguments. The initial value for `x` will then be of the type `x: Sequence[float]`. You might need the unpacking\n",
    "operator `*` for testing your implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimization of continuous functions\n",
    "\n",
    "Many drawbacks arose because we ignored the continuous nature of `f` and just used a discrete search with a fixed\n",
    "step size. What one can do instead is to use derivatives for finding the right direction to go to and an adjustable\n",
    "step size simultaneously.\n",
    "\n",
    "We have then discussed what exactly derivatives are and how to use them for optimization. The resulting algorithm\n",
    "is called _gradient descent_.\n",
    "\n",
    "### Homework 2\n",
    "Especially for those missing last time: please read up on gradient descent. We will discuss it in detail next time\n",
    "but the discussion will be easier if you are prepared. Also, read as much as possible about derivatives/gradients\n",
    "and about linear approximations of continuous functions.\n",
    "\n",
    "Bonus: try to implement gradient descent on your own for some functions of your choice where you know the derivatives\n",
    "(polynomials would be a good start)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparation for session 3\n",
    "\n",
    "We will go into more detail about gradient descent, derivatives, optimization and linear models next time.\n",
    "Python will be our companion for the entirety of the journey, and it is important to master the tool one is using.\n",
    "I highly recommend you to play around, read and implement as much as you can in preparation for the next class.\n",
    "\n",
    "For example, [tutorialspoint](https://www.tutorialspoint.com/python/index.htm) and\n",
    "[w3schools](https://www.w3schools.com/python/) are good places to read about\n",
    "basic definitions and concepts. There are many more tutorials out there if you prefer something else.\n",
    "\n",
    "[Hackerrank](https://www.hackerrank.com/) and similar pages are awesome resources for exercises of various difficulties.\n",
    "I suggest that you create and account there and start solving some problems in python. You can also find tons\n",
    "of beginners exercises for familiarizing yourselves with syntax, features, data structures and standard libraries.\n",
    "Get your googling on ;).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}