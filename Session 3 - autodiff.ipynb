{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Callable, Tuple"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Gradient descent and automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 1:\n",
    "\n",
    "Write an algorithm for gradient descent using the following scheme and try it on some functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "def gradient_descent_1dim(\n",
    "    f: Callable[[float], float],\n",
    "    grad_f: Callable[[float], float],\n",
    "    num_steps=50,\n",
    "    step_size=1e-4,\n",
    "    initial_x: float = 0\n",
    ") -> Tuple[float, float]:\n",
    "    stop_at_gradient_below = 1e-3\n",
    "    current_pos = initial_x\n",
    "    for n in range(num_steps):\n",
    "        ...\n",
    "\n",
    "    print(f\"Failed to converge after {num_steps} steps, returning current candidates for argmin and min\")\n",
    "    return current_pos, f(current_pos)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def f1(x: float):\n",
    "    return (6*x -2)**4 - 2*x\n",
    "\n",
    "\n",
    "def grad_f1(x: float):\n",
    "    return 4 * ((6*x - 2)**3) * 6 - 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to converge after 1000 steps, returning current candidates for argmin and min\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0, 16)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent_1dim(f1, grad_f1, num_steps=1000, step_size=2e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our main problem is that we need to pass the gradient on our own. Instead, we would like to just pass a function\n",
    "and the gradient to be computed automatically / numerically for us. The solution that we are going to use is at\n",
    "the very core of most machine learning libraries - _autodiff_. Essentially, it is just the use of the chain rule\n",
    "and the computation of complicated functions by composition of simple functions.\n",
    "\n",
    "Our presentation is largely based on [this post](https://sidsite.com/posts/autodiff/)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    def __init__(self, value: float, local_gradients=()):\n",
    "        self.value = value\n",
    "        self.local_gradients = local_gradients\n",
    "\n",
    "def add(a: Variable, b: Variable):\n",
    "    \"\"\"Create the variable that results from adding two variables.\"\"\"\n",
    "    value = a.value + b.value\n",
    "    local_gradients = (\n",
    "        (a, 1),  # the local derivative with respect to a is 1\n",
    "        (b, 1)   # the local derivative with respect to b is 1\n",
    "    )\n",
    "    return Variable(value, local_gradients)\n",
    "\n",
    "def mul(a: Variable, b: Variable):\n",
    "    \"\"\"Create the variable that results from multiplying two variables.\"\"\"\n",
    "    value = a.value * b.value\n",
    "    local_gradients = (\n",
    "        (a, b.value), # the local derivative with respect to a is b.value\n",
    "        (b, a.value)  # the local derivative with respect to b is a.value\n",
    "    )\n",
    "    return Variable(value, local_gradients)\n",
    "\n",
    "\n",
    "def get_gradients(variable: Variable):\n",
    "    \"\"\" Compute the first derivatives of `variable`\n",
    "    with respect to child variables.\n",
    "    \"\"\"\n",
    "    gradients = defaultdict(lambda: 0)\n",
    "\n",
    "    def compute_gradients_recursively(var: Variable, path_value: float):\n",
    "        for child_variable, local_gradient in var.local_gradients:\n",
    "            # Multiply the edges of a path\n",
    "            value_of_path_to_child = path_value * local_gradient\n",
    "            # Add together the different paths\n",
    "            gradients[child_variable] += value_of_path_to_child\n",
    "            # recurse through graph\n",
    "            compute_gradients_recursively(child_variable, value_of_path_to_child)\n",
    "\n",
    "    compute_gradients_recursively(variable, path_value=1)\n",
    "    # (path_value=1 is from `variable` differentiated w.r.t. itself)\n",
    "    return dict(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Note:\n",
    "\n",
    "The above code may have elements that are new to you: classes, defaultdict are recursive functions. Make sure you\n",
    "understand them before you proceed (or ask me for support).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "a = Variable(4)\n",
    "b = Variable(12)\n",
    "\n",
    "d = mul(add(a, b), add(a, b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "get_gradients(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 2:\n",
    "\n",
    "Implement gradient descent with autodiff for simple functions. Add more possibilities like powers, multiplication\n",
    "with a constant (not actually done in the link above) and use python magic for overloading operators.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}